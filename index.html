<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="Manipulate by Seeing: Creating Manipulation Controllers from Pre-Trained Representations">
  <meta name="keywords" content="Robot Learning, Visual Learning, Manipuation">
  <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property="og:site_name" content="Manipulate by Seeing: Creating Manipulation Controllers from Pre-Trained Representations" />
    <meta property="og:type" content="video.other" />
    <meta property="og:title" content="Manipulate by Seeing: Creating Manipulation Controllers from Pre-Trained Representations" />
    <meta property="og:description" content="Manipulate by Seeing: Creating Manipulation Controllers from Pre-Trained Representations" />
    <meta property="og:url" content="https://AGI-Labs.github.io/manipulate-by-seeing" />
    <meta property="og:image" content="" />
    <meta property="og:image:secure" content="" />
    <meta property="og:video" content="" />
    <meta property="og:video:secure" content="" />

    <meta property="article:publisher" content="https://AGI-Labs.github.io/manipulate-by-seeing" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Manipulate by Seeing: Creating Manipulation Controllers from Pre-Trained Representations" />
    <meta name="twitter:description" content="Manipulate by Seeing: Creating Manipulation Controllers from Pre-Trained Representations" />
    <meta name="twitter:url" content="" />
    <meta name="twitter:image" content="" />
    <meta name="twitter:site" content="" />
    <meta property="og:image:width" content="1600" />
    <meta property="og:image:height" content="900" />

    <script src="http://www.youtube.com/iframe_api"></script>
    <meta name="twitter:card" content="player" />
    <meta name="twitter:image" content="" />
    <meta name="twitter:player" content="" />
    <meta name="twitter:player:width" content="640" />
    <meta name="twitter:player:height" content="360" />



  <title>Manipulate by Seeing: Creating Manipulation Controllers from Pre-Trained Representations</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <meta property="og:image:width" content="1600" />
  <meta property="og:image:height" content="900" />

  <script src="https://www.youtube.com/iframe_api"></script>
  <meta name="twitter:card" content="player" />
  <meta name="twitter:image" content="" />
  <meta name="twitter:player:width" content="640" />
  <meta name="twitter:player:height" content="360" />

</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Manipulate by Seeing: Creating Manipulation Controllers from Pre-Trained Representations</h2>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://www.jianrenw.com/">Jianren Wang*</a><sup></sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
              <a href="https://sudeepdasari.github.io/">Sudeep Dasari*</a><sup></sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
              <a href="https://www.mohansrirama.com/">Mohan Kumar</a><sup></sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
              <a href="http://shubhtuls.github.io/">Shubham Tulsiani</a><sup></sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
              <a href="http://www.cs.cmu.edu/~abhinavg/">Abhinav Gupta</a><sup></sup>
              <br /><sup></sup>Carnegie Mellon University<br />
            <h6 style = "font-size:14px">*denotes equal contribution</h6>

          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2303.08135"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://agi-labs.github.io/manipulate-by-seeing/resources/teaser.mp4"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <span class="link-block">
                <a href=https://github.com/AGI-Labs/manipulate-by-seeing
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>
              <!-- <span class="link-block">
                <a target="_blank" href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-twitter"></i>
                  </span>
                  <span>Summary</span>
                </a>
              </span>
              <span class="link-block">
                <a href=resources/TK_Poster.pdf
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Poster</span>
                </a>
              </span> -->

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      
    <!--/ Abstract. -->

    <div class="columns is-centered has-text-centered">
      <div class="column is-max-desktop">
        <div class="publication-video">
          <iframe src="resources/teaser.mp4" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The field of visual representation learning has seen explosive growth in the past years, but its benefits in robotics have been surprisingly limited so far. Prior work uses generic visual representations as a basis to learn (task-specific) robot action policies (e.g. via behavior cloning). While the visual representations do accelerate learning, they are primarily used to encode visual observations. Thus, action information has to be derived purely from robot data, which is expensive to collect! In this work, we present a scalable alternative where the visual representations can help directly infer robot actions. We observe that vision encoders express relationships between image observations as \textit{distances} (e.g. via embedding dot product) that could be used to efficiently plan robot behavior. We operationalize this insight and develop a simple algorithm for acquiring a distance function and dynamics predictor, by fine-tuning a pre-trained representation on human collected video sequences. The final method is able to substantially outperform traditional robot learning baselines (e.g. 70% success v.s. 50% for behavior cloning on pick-place) on a suite of diverse real-world manipulation tasks. It can also generalize to novel objects, without using any robot demonstrations during train time.
        </div>
      </div>
    </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-2" style="text-align: center;">Setting</h2>
        <tr>
          <td>
            <br>
            <div class="columns is-centered has-text-centered">
              <img src="./resources/prob_setting.jpg"></img>
            </div>
          </td>
        </tr>
        <div class="is-vcentered interpolation-panel">
          <div class="content has-text-centered">
            <p style = "font-size: 18px">
              <b>Our system:</b> In our problem setting we use a low-cost reacher grabber tool (left) to collect training demonstrations. These demonstrations are used to acquire a robot controller purely through distance/representation learning. The final system is deployed on a robot (right) to solve various tasks at test-time.
            </p>
          </div>
        </div>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-2" style="text-align: center;">Training</h2>
        <tr>
          <td>
            <br>
            <div class="columns is-centered has-text-centered">
              <img src="./resources/loss.jpg"></img>
            </div>
          </td>
        </tr>
        <div class="is-vcentered interpolation-panel">
          <div class="content has-text-centered">
            <p style = "font-size: 18px">
              Our method leverages a pre-trained representation network, R, to encode observations, i<sub>t</sub> = R(I<sub>t</sub>), and enables control via distance learning. Specifically, we use contrastive representation learning methods to learn a distance metric, d(i<sub>j</sub>, i<sub>k</sub>), on the pre-trained embedding space. The key idea is to use this distance metric to select which of the possible future state is closest to the goal state. But how do we predict possible future states? We explicitly learn a dynamics function, F(i<sub>t</sub>, a<sub>t</sub>) that predicts future state for a possible action a<sub>t</sub>. During test time, we predict multiple future states using different possible action and select the one which is closes to goal state.
            </p>
          </div>
        </div>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-2" style="text-align: center;">Testing</h2>
        <tr>
          <td>
            <br>
            <div class="columns is-centered has-text-centered">
              <img src="./resources/test.gif"></img>
            </div>
          </td>
        </tr>
        <div class="is-vcentered interpolation-panel">
          <div class="content has-text-centered">
            <p style = "font-size: 18px">
              Given the distance function and dynamics model, our inference procedure is as simple as choosing the action that minimizes the distance to the goal state. More concretely, initialize from a beginning state and given a goal image. We consider a set of candidate actions at each step. The learned distance predictor then infers the future distance corresponding to all these candidates, and we execute the action with the lowest predicted distance-to-goal. We repeat this procedure until reaching sufficiently close to the desired goal.
            </p>
          </div>
        </div>
      </div>
    </div>
  
    <h2 class="title is-2" style="text-align: center;">Experiment Videos</h2>
        <br>
            <video width="640" height="480" loop muted controls picture-in-picture>
              <source src="./resources/pushing_1.mp4" type="video/mp4">
            </video>
            <video width="640" height="480" loop muted controls picture-in-picture>
              <source src="./resources/stack_1.mp4" type="video/mp4">
            </video>
          <br/>
          <video width="640" height="480" loop muted controls picture-in-picture>
            <source src="./resources/open_1.mp4" type="video/mp4"  >
          </video>
          <video width="640" height="480" loop muted controls picture-in-picture>
            <source src="./resources/knob_1.mp4" type="video/mp4">
          </video>
    
  </div>
</section>

<div class="is-vcentered interpolation-panel">
  <!-- <div class="content has-text-centered">
    <p style = "font-size: 16px">
      We are grateful to Bernie Zhu, Yuchu Zhang and Yuyang Wang for initial feedback on the paper. We would like to thank Joyce Hong, Yuyang Wang for helping with data collect.
      JW is supported by DARPAMCS, ONR Young Investigator, ONR muri. SD is supported by
    </p>
  </div>
</div> -->

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a href="./resources/CoRL-Energy-Locomotion.pdf" class="large-font bottom_buttons">
        <i class="fas fa-file-pdf"></i>
      </a>
      <!-- <a class="large-font bottom_buttons" disabled>
        <i class="fab fa-github"></i>
      </a> -->
      <br />
      <p>Page template borrowed from <a href="https://nerfies.github.io"><span class="dnerf">Nerfies</span></a> and <a href="https://energy-locomotion.github.io/"><span class="dnerf">Energy Locomotion</span></a>.</p>
    </div>
  </div>
</footer>

</body>
</html>
