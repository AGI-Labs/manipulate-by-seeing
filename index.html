<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="Learning to Manipulate by Learning to See">
  <meta name="keywords" content="Robot Learning, Visual Learning, Manipuation">
  <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property="og:site_name" content="Learning to Manipulate by Learning to See" />
    <meta property="og:type" content="video.other" />
    <meta property="og:title" content="Learning to Manipulate by Learning to See" />
    <meta property="og:description" content="Learning to Manipulate by Learning to See" />
    <meta property="og:url" content="https://AGI-Labs.github.io/manipulate-by-seeing" />
    <meta property="og:image" content="" />
    <meta property="og:image:secure" content="" />
    <meta property="og:video" content="" />
    <meta property="og:video:secure" content="" />

    <meta property="article:publisher" content="https://AGI-Labs.github.io/manipulate-by-seeing" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Learning to Manipulate by Learning to See" />
    <meta name="twitter:description" content="Learning to Manipulate by Learning to See" />
    <meta name="twitter:url" content="" />
    <meta name="twitter:image" content="" />
    <meta name="twitter:site" content="" />
    <meta property="og:image:width" content="1600" />
    <meta property="og:image:height" content="900" />

    <script src="http://www.youtube.com/iframe_api"></script>
    <meta name="twitter:card" content="player" />
    <meta name="twitter:image" content="" />
    <meta name="twitter:player" content="" />
    <meta name="twitter:player:width" content="640" />
    <meta name="twitter:player:height" content="360" />



  <title>Learning to Manipulate by Learning to See</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <meta property="og:image:width" content="1600" />
  <meta property="og:image:height" content="900" />

  <script src="https://www.youtube.com/iframe_api"></script>
  <meta name="twitter:card" content="player" />
  <meta name="twitter:image" content="" />
  <meta name="twitter:player:width" content="640" />
  <meta name="twitter:player:height" content="360" />

</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Learning to Manipulate by Learning to See</h2>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://www.jianrenw.com/">Jianren Wang*</a><sup></sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
              <a href="https://sudeepdasari.github.io/">Sudeep Dasari*</a><sup></sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
              <a href="https://www.mohansrirama.com/">Mohan Kumar</a><sup></sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
              <a href="http://shubhtuls.github.io/">Shubham Tulsiani</a><sup></sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
              <a href="http://www.cs.cmu.edu/~abhinavg/">Abhinav Gupta</a><sup></sup>
              <br /><sup></sup>Carnegie Mellon University<br />
            <span>In submission CVPR 2023</span>
            <h6 style = "font-size:14px">*denotes equal contribution</h6>

          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>

              <span class="link-block">
                <a target="_blank" href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-twitter"></i>
                  </span>
                  <span>Summary</span>
                </a>
              </span>
              <span class="link-block">
                <a href=resources/TK_Poster.pdf
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Poster</span>
                </a>
              </span>

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      
    <!--/ Abstract. -->

    <div class="columns is-centered has-text-centered">
      <div class="column is-max-desktop">
        <div class="publication-video">
          <iframe src="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            While the field of visual representation learning has seen explosive growth in the past years, its spillover effects in robotics have been surprisingly limited so far. Prior work in this space used pre-trained representations to ``warm start" policy and value function learning. This approach is inherently limited, since predicting action sequences and learning value functions both require finicky algorithms and hard-to-tune networks. Instead, we propose to abandon this paradigm and directly leverage representations to control the robotic manipulator. This is made possible by the structure of the learned visual representation space: it naturally encodes relationships between states as distances. These distances can be used to plan for robot behavior by greedily selecting actions that best reach a goal state. This paper develops a simple algorithm for acquiring a distance function and dynamics predictor from pre-trained visual representations, using human collected video sequences. In addition to outperforming baselines that adopt the tradition action/value learning perspective (e.g. our method gets 90% success v.s. 65% for behavior cloning on pushing task), this approach can acquire manipulation controllers without any robot demonstrations or rollouts.
        </div>
      </div>
    </div>
    </div>
  </div>
</section>

    <div class="is-vcentered interpolation-panel"> 
      <h2 class="title is-2" style="text-align: center;">Short Talk</h2>
      <div class="content has-text-centered"><div class="vsc-controller"></div>
        <video id="remote" width="960" height="720" controls playsinline picture-in-picture style="border: 1px solid #bbb; border-radius: 10px;">
          <source src="" type="video/mp4">
        </video>
      </div>
    </div>

<section class="section">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-2" style="text-align: center;">Setting</h2>
        <tr>
          <td>
            <br>
            <div class="columns is-centered has-text-centered">
              <img src="./resources/prob_setting.jpg"></img>
            </div>
          </td>
        </tr>
        <div class="is-vcentered interpolation-panel">
          <div class="content has-text-centered">
            <p style = "font-size: 18px">
              <b>Our system:</b> In our problem setting we use a low-cost reacher grabber tool (left) to collect training demonstrations. These demonstrations are used to acquire a robot controller purely through distance/representation learning. The final system is deployed on a robot (right) to solve various tasks at test-time.
            </p>
          </div>
        </div>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-2" style="text-align: center;">Training</h2>
        <tr>
          <td>
            <br>
            <div class="columns is-centered has-text-centered">
              <img src="./resources/loss.jpg"></img>
            </div>
          </td>
        </tr>
        <div class="is-vcentered interpolation-panel">
          <div class="content has-text-centered">
            <p style = "font-size: 18px">
              Our method leverages a pre-trained representation network, R, to encode observations, i<sub>t</sub> = R(I<sub>t</sub>), and enables control via distance learning. Specifically, we use contrastive representation learning methods to learn a distance metric, d(i<sub>j</sub>, i<sub>k</sub>), on the pre-trained embedding space. The key idea is to use this distance metric to select which of the possible future state is closest to the goal state. But how do we predict possible future states? We explicitly learn a dynamics function, F(i<sub>t</sub>, a<sub>t</sub>) that predicts future state for a possible action a<sub>t</sub>. During test time, we predict multiple future states using different possible action and select the one which is closes to goal state.
            </p>
          </div>
        </div>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-2" style="text-align: center;">Testing</h2>
        <tr>
          <td>
            <br>
            <div class="columns is-centered has-text-centered">
              <img src="./resources/test.gif"></img>
            </div>
          </td>
        </tr>
        <div class="is-vcentered interpolation-panel">
          <div class="content has-text-centered">
            <p style = "font-size: 18px">
              Given the distance function and dynamics model, our inference procedure is as simple as choosing the action that minimizes the distance to the goal state. More concretely, initialize from a beginning state and given a goal image. We consider a set of candidate actions at each step. The learned distance predictor then infers the future distance corresponding to all these candidates, and we execute the action with the lowest predicted distance-to-goal. We repeat this procedure until reaching sufficiently close to the desired goal.
            </p>
          </div>
        </div>
      </div>
    </div>
  
    <h2 class="title is-2" style="text-align: center;">Experiment Videos</h2>
        <br>
            <video width="640" height="480" loop muted controls picture-in-picture>
              <source src="./resources/" type="video/mp4">
            </video>
            <video width="640" height="480" loop muted controls picture-in-picture>
              <source src="./resources/" type="video/mp4">
            </video>
          <br/>
          <video width="640" height="480" loop muted controls picture-in-picture>
            <source src="./resources/" type="video/mp4"  >
          </video>
          <video width="640" height="480" loop muted controls picture-in-picture>
            <source src="./resources/" type="video/mp4">
          </video>
    
  </div>
</section>

<div class="is-vcentered interpolation-panel">
  <div class="content has-text-centered">
    <p style = "font-size: 16px">
      We are grateful to Bernie Zhu, Yuchu Zhang and Yuyang Wang for initial feedback on the paper. We would like to thank Joyce Hong, Yuyang Wang for helping with data collect.
      JW is supported by DARPAMCS, ONR Young Investigator, ONR muri. SD is supported by
    </p>
  </div>
</div>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a href="./resources/CoRL-Energy-Locomotion.pdf" class="large-font bottom_buttons">
        <i class="fas fa-file-pdf"></i>
      </a>
      <!-- <a class="large-font bottom_buttons" disabled>
        <i class="fab fa-github"></i>
      </a> -->
      <br />
      <p>Page template borrowed from <a href="https://nerfies.github.io"><span class="dnerf">Nerfies</span></a> and <a href="https://energy-locomotion.github.io/"><span class="dnerf">Energy Locomotion</span></a>.</p>
    </div>
  </div>
</footer>

</body>
</html>
